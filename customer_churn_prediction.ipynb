{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Customer Churn \n",
    "\n",
    "In this project we will be trying to predict customer churn by analyzing customer data and building machine learning models to predict which customers are likely to churn. We will preprocess the data and perform EDA to gain some insight into the overall dataset. We will perform feature engineering to create new features to enhance the quality of the models. We will split our data in to 3 sets via a 60/20/20 split for training, validation, and testing sets. We will then proceed to train atleast two models, one logistic regression to use a baseline, and atleast one boosting model, which we hope with perform better. We will tune hyperparameters via cross validation grids and refine our models until we acheive an AUC-ROC score > 88%, at which point we will use our best performing model on the final test data and submit a final report with our results and findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m contract_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<https://drive.google.com/file/d/1oReKO7p6kSzQOvqghUDyZcE0U1HftuTL/view?usp=sharing>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m personal_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<https://drive.google.com/file/d/1KSUGtbd69khobixB-Qbw-MV78V2wLIrn/view?usp=sharing>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m internet_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<https://drive.google.com/file/d/1IQtynYs_m9Z3Bcef7lmxq9Dhuuqyw7dX/view?usp=sharing>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "contract_df = pd.read_csv('/datasets/final_provider/contract.csv')\n",
    "personal_df = pd.read_csv('/datasets/final_provider/personal.csv')\n",
    "internet_df = pd.read_csv('/datasets/final_provider/internet.csv')\n",
    "phone_df = pd.read_csv('/datasets/final_provider/phone.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(contract_df.head())\n",
    "print(contract_df.isnull().sum())\n",
    "print(contract_df['customerID'].duplicated().sum())\n",
    "\n",
    "display(personal_df.head())\n",
    "print(personal_df.isnull().sum())\n",
    "print(personal_df['customerID'].duplicated().sum())\n",
    "\n",
    "display(internet_df.head())\n",
    "print(internet_df.isnull().sum())\n",
    "print(internet_df['customerID'].duplicated().sum())\n",
    "\n",
    "display(phone_df.head())\n",
    "print(phone_df.isnull().sum())\n",
    "print(phone_df['customerID'].duplicated().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is no null values or duplicated values in any of our datasets which is great, but there is some obvious issues with the data that need to be addressed. First, to facilitate processing, we will merge the datasets using left join. Then we will convert the column names to lowercase and snake case to avoid confusion later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = contract_df.merge(personal_df, on='customerID', how='left')\n",
    "\n",
    "df = df.merge(internet_df, on='customerID', how='left')\n",
    "\n",
    "df = df.merge(phone_df, on='customerID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(name):\n",
    "    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)  \n",
    "    name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name) \n",
    "    return name.lower()\n",
    "\n",
    "df.columns = [to_snake_case(col) for col in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will check the dataset to confirm the column names have been converted properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will encode binary features as well as multi class features and create a new \"Churn\" column using the values from end date column which will have binary value for yes/no churn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_map = {\n",
    "    'Yes': 1,\n",
    "    'No': 0,\n",
    "    'Female': 0,\n",
    "    'Male': 1\n",
    "}\n",
    "\n",
    "binary_cols = [\n",
    "    'paperless_billing', 'partner', 'dependents', 'gender',\n",
    "    'online_security', 'online_backup', 'device_protection',\n",
    "    'tech_support', 'streaming_tv', 'streaming_movies', 'multiple_lines'\n",
    "]\n",
    "\n",
    "df[binary_cols] = df[binary_cols].replace(binary_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class_cols = ['payment_method', 'internet_service', 'type']\n",
    "\n",
    "df = pd.get_dummies(df, columns=multi_class_cols, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['churn'] = df['end_date'].apply(lambda x: 0 if x == 'No' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['begin_date'] = pd.to_datetime(df['begin_date'])\n",
    "df['end_date'] = pd.to_datetime(df['end_date'], errors='coerce')\n",
    "df['total_charges'] = pd.to_numeric(df['total_charges'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform more detailed EDA to get a better idea of our data and try to derive some basic insights before moving on to model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "display(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['churn'].value_counts(normalize=True).plot(kind='bar', title='Churn Distribution')\n",
    "\n",
    "len(contract_df[contract_df['EndDate'] == 'No']) / len(contract_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see about 27% of customers churned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['monthly_charges', 'total_charges']\n",
    "\n",
    "for col in num_cols:\n",
    "    plt.figure()\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'{col} Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the charts that majority of customers have lower cost plans, with count of users highest at the bottom end, then increasing slightly until about the $80 range and decreasing again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'paperless_billing', 'partner', 'dependents', 'gender',\n",
    "    'online_security', 'online_backup', 'device_protection',\n",
    "    'tech_support', 'streaming_tv', 'streaming_movies', 'multiple_lines'\n",
    "]\n",
    "\n",
    "for col in cat_cols:\n",
    "    plt.figure()\n",
    "    sns.barplot(x=col, y='churn', data=df)\n",
    "    plt.title(f'Churn Rate by {col}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see there is a strong connection of churn rate with paperless billing, tech support, and online security. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "numeric_df = df.select_dtypes(include=['int64', 'float64']) \n",
    "sns.heatmap(numeric_df.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will look for relationships between churn and date. We will check if there is higher chrun on specific months or if there is a correlation between begindate months and churn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.Timestamp('2020-02-01')\n",
    "df['tenure_days'] = (df['end_date'].fillna(today) - df['begin_date']).dt.days\n",
    "\n",
    "df['churn_month'] = df['end_date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_months = pd.Index(range(1, 13), name='churn_month')\n",
    "\n",
    "churn_by_month = (\n",
    "    df[df['end_date'].notna()]\n",
    "    .groupby('churn_month')\n",
    "    .size()\n",
    "    .reindex(all_months, fill_value=0)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=churn_by_month.index, y=churn_by_month.values)\n",
    "plt.title(\"Number of Churns by Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Churn Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['signup_month'] = pd.to_datetime(df['begin_date'], errors='coerce').dt.month\n",
    "\n",
    "signup_churn = (\n",
    "    df[df['churn'] == 1]\n",
    "    .groupby('signup_month')\n",
    "    .size()\n",
    "    .reindex(range(1, 13), fill_value=0) \n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=signup_churn.index, y=signup_churn.values)\n",
    "plt.title(\"Number of Churns by Signup Month (BeginDate)\")\n",
    "plt.xlabel(\"Signup Month\")\n",
    "plt.ylabel(\"Churn Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is indeed a correlation between begindate (signup month) and churn, with higher churn in customers who sign up in the winter months. We will use this for feature enginering so we can include the signup month as a feature in our models. We will use only begindate and not end date as that would lead to data leakage, considering any customer with an end date would obviously have a positive value for \"churn\", defeating the purpose of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['signup_month'] = pd.to_datetime(df['begin_date'], errors='coerce').dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will begin training our models. First we will train a simple Logistic Regression model to use as a baseline and split the data into 60/20/20 for trianing, validation, and final testing. We will encode the feature variables to facilitate the processing. Moving forward we will try atleast one boosting model to see if we can improve the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['customer_id', 'begin_date', 'end_date', 'churn_month']\n",
    "X = df.drop(columns=drop_cols + ['churn'])\n",
    "y = df['churn']\n",
    "\n",
    "service_cols = [\n",
    "    'tech_support', 'online_security', 'online_backup',\n",
    "    'device_protection', 'streaming_tv', 'streaming_movies', 'multiple_lines'\n",
    "]\n",
    "X[service_cols] = X[service_cols].fillna('No')\n",
    "\n",
    "X['total_charges'] = X['total_charges'].fillna(X['total_charges'].median())\n",
    "\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes[X.dtypes == 'object']\n",
    "X.isnull().sum().sort_values(ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_proba = model.predict_proba(X_val)[:, 1] \n",
    "\n",
    "print(\"Validation Results:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(f\"AUC-ROC Score: {roc_auc_score(y_val, y_val_proba):.4f}\")\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_proba)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_score(y_val, y_val_proba):.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Validation Set)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Regression model is performing quite well already, now we will attempt to further improve the results by training more advanced models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [50, 100],\n",
    "    'scale_pos_weight': [1, 2]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=6,         \n",
    "    scoring='roc_auc',\n",
    "    cv=2,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "y_val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"Validation Results (Tuned XGBoost - Fast):\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(f\"AUC-ROC Score: {roc_auc_score(y_val, y_val_proba):.4f}\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_val, y_val_proba)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_score(y_val, y_val_proba):.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Tuned XGBoost - Fast)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we achieved excellent results, with an AUC-ROC score of 94%! We used an XG Boost model which achieves very strong predictive power by building ensembles of decision trees and correcting mistakes of previous generations. We selected hyperparameters for a good balance of efficiency and performance, and made sure to include scale_pos_weight to balance the classes, since we had many more non-churners in the data than churners. We used cross validation to select the best overall combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_model.predict(X_test)\n",
    "y_test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Test Results (Final XGBoost Model):\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(f\"AUC–ROC Score (Test Set): {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Test Set)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we achieve a final test AUC-ROC score of 94% ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "Over the course of our analysis we gained some important insights into our data. We found that our hunch was correct and there was in fact a correlation between begin/end dates and churn. We found that customers who signed up during winter months were far more likely to churn, and that churns mostly happen in the winter months as well. We then used this information in our feature engineering to improve the performance of our models. For the model training we started with a simple logistic regression as a baseline and then moved on to a more advanced XG Boost model. We selected hyperparameters that would be efficient enough to complete in reasonable time while providing the best possible performance, and used a CV grid to select the best combination of parameters. We made sure to balance the classes in our final model and achieved an excellent AUC-ROC score of 94% ! Overall, we found our model accurately predicted churn in users. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
